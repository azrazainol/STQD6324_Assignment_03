{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azrazainol/STQD6324_Assignment_03/blob/main/Copy_of_Assignment3_P137262.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlF3EdCScbe7",
        "outputId": "4f40d424-8e2a-42f7-8e4c-67e5051b8885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ca8ee148c8990edf801c83590d1feab539689dd4a2696e2b9bfc788d15d18392\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"IrisClassification\").getOrCreate()\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "iris_df['label'] = iris.target\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "df = spark.createDataFrame(iris_df)\n",
        "\n",
        "# Feature Engineering\n",
        "assembler = VectorAssembler(inputCols=iris.feature_names, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Index labels\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
        "df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Define Random Forest classifier\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", seed=42)\n",
        "\n",
        "# Set up pipeline for Random Forest\n",
        "pipeline_rf = Pipeline(stages=[rf])\n",
        "\n",
        "# Create parameter grid for Random Forest\n",
        "paramGrid_rf = (ParamGridBuilder()\n",
        "                .addGrid(rf.numTrees, [10, 15, 20, 25, 30])  # Number of trees in the forest\n",
        "                .addGrid(rf.maxDepth, [5, 7, 10, 12, 15])   # Maximum depth of the tree\n",
        "                .build())\n",
        "\n",
        "# Define cross-validator for Random Forest\n",
        "crossval_rf = CrossValidator(estimator=pipeline_rf,\n",
        "                             estimatorParamMaps=paramGrid_rf,\n",
        "                             evaluator=MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\"),\n",
        "                             numFolds=5, seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Train the Random Forest model using cross-validation\n",
        "cvModel_rf = crossval_rf.fit(train_data)\n",
        "\n",
        "# Print best model parameters for Random Forest\n",
        "best_rf_model = cvModel_rf.bestModel.stages[-1]\n",
        "print(\"Random Forest - Best Model Parameters:\")\n",
        "print(best_rf_model.extractParamMap())\n",
        "print()\n",
        "\n",
        "# Make predictions on the test data using Random Forest\n",
        "predictions_rf = cvModel_rf.transform(test_data)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "evaluator_rf = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\")\n",
        "accuracy_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "evaluator_rf.setMetricName(\"weightedPrecision\")\n",
        "precision_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "evaluator_rf.setMetricName(\"weightedRecall\")\n",
        "recall_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "evaluator_rf.setMetricName(\"f1\")\n",
        "f1_score_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "# Print evaluation metrics for Random Forest\n",
        "print(\"Random Forest Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_rf}\")\n",
        "print(f\"Precision: {precision_rf}\")\n",
        "print(f\"Recall: {recall_rf}\")\n",
        "print(f\"F1 Score: {f1_score_rf}\")\n",
        "print()\n",
        "\n",
        "# Compute confusion matrix for Random Forest\n",
        "y_true_rf = predictions_rf.select(\"indexedLabel\").toPandas()\n",
        "y_pred_rf = predictions_rf.select(\"prediction\").toPandas()\n",
        "\n",
        "cm_rf = confusion_matrix(y_true_rf, y_pred_rf)\n",
        "print(\"Confusion Matrix (Random Forest):\\n\", cm_rf)\n",
        "print()\n",
        "\n",
        "# Define Decision Tree classifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", seed=42)\n",
        "\n",
        "# Set up pipeline for Decision Tree\n",
        "pipeline_dt = Pipeline(stages=[dt])\n",
        "\n",
        "# Create parameter grid for Decision Tree\n",
        "paramGrid_dt = (ParamGridBuilder()\n",
        "                .addGrid(dt.maxDepth, [5, 7, 10, 12, 15])  # Maximum depth of the tree\n",
        "                .build())\n",
        "\n",
        "# Define cross-validator for Decision Tree\n",
        "crossval_dt = CrossValidator(estimator=pipeline_dt,\n",
        "                             estimatorParamMaps=paramGrid_dt,\n",
        "                             evaluator=MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\"),\n",
        "                             numFolds=5, seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Train the Decision Tree model using cross-validation\n",
        "cvModel_dt = crossval_dt.fit(train_data)\n",
        "\n",
        "# Print best model parameters for Decision Tree\n",
        "best_dt_model = cvModel_dt.bestModel.stages[-1]\n",
        "print(\"Decision Tree - Best Model Parameters:\")\n",
        "print(best_dt_model.extractParamMap())\n",
        "print()\n",
        "\n",
        "# Make predictions on the test data using Decision Tree\n",
        "predictions_dt = cvModel_dt.transform(test_data)\n",
        "\n",
        "# Evaluate Decision Tree model\n",
        "evaluator_dt = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\")\n",
        "accuracy_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "evaluator_dt.setMetricName(\"weightedPrecision\")\n",
        "precision_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "evaluator_dt.setMetricName(\"weightedRecall\")\n",
        "recall_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "evaluator_dt.setMetricName(\"f1\")\n",
        "f1_score_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "# Print evaluation metrics for Decision Tree\n",
        "print(\"Decision Tree Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_dt}\")\n",
        "print(f\"Precision: {precision_dt}\")\n",
        "print(f\"Recall: {recall_dt}\")\n",
        "print(f\"F1 Score: {f1_score_dt}\")\n",
        "print()\n",
        "\n",
        "# Compute confusion matrix for Decision Tree\n",
        "y_true_dt = predictions_dt.select(\"indexedLabel\").toPandas()\n",
        "y_pred_dt = predictions_dt.select(\"prediction\").toPandas()\n",
        "\n",
        "cm_dt = confusion_matrix(y_true_dt, y_pred_dt)\n",
        "print(\"Confusion Matrix (Decision Tree):\\n\", cm_dt)\n",
        "print()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bfp9rhNjdUv",
        "outputId": "a94e2f54-be4a-4dcc-c6f5-d02cdaecb4a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest - Best Model Parameters:\n",
            "{Param(parent='RandomForestClassifier_45ceb34aa731', name='bootstrap', doc='Whether bootstrap samples are used when building trees.'): True, Param(parent='RandomForestClassifier_45ceb34aa731', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False, Param(parent='RandomForestClassifier_45ceb34aa731', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10, Param(parent='RandomForestClassifier_45ceb34aa731', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'auto', Param(parent='RandomForestClassifier_45ceb34aa731', name='featuresCol', doc='features column name.'): 'features', Param(parent='RandomForestClassifier_45ceb34aa731', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini', Param(parent='RandomForestClassifier_45ceb34aa731', name='labelCol', doc='label column name.'): 'indexedLabel', Param(parent='RandomForestClassifier_45ceb34aa731', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '', Param(parent='RandomForestClassifier_45ceb34aa731', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32, Param(parent='RandomForestClassifier_45ceb34aa731', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='RandomForestClassifier_45ceb34aa731', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256, Param(parent='RandomForestClassifier_45ceb34aa731', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='RandomForestClassifier_45ceb34aa731', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='RandomForestClassifier_45ceb34aa731', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0, Param(parent='RandomForestClassifier_45ceb34aa731', name='numTrees', doc='Number of trees to train (>= 1).'): 10, Param(parent='RandomForestClassifier_45ceb34aa731', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='RandomForestClassifier_45ceb34aa731', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='RandomForestClassifier_45ceb34aa731', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='RandomForestClassifier_45ceb34aa731', name='seed', doc='random seed.'): 42, Param(parent='RandomForestClassifier_45ceb34aa731', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.9821428571428571\n",
            "Precision: 0.9835164835164836\n",
            "Recall: 0.9821428571428572\n",
            "F1 Score: 0.9822586872586874\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            " [[25  0  0]\n",
            " [ 0 12  0]\n",
            " [ 0  1 18]]\n",
            "\n",
            "Decision Tree - Best Model Parameters:\n",
            "{Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False, Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10, Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='featuresCol', doc='features column name.'): 'features', Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini', Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='labelCol', doc='label column name.'): 'indexedLabel', Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '', Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32, Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256, Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0, Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='DecisionTreeClassifier_d8ab216df5ef', name='seed', doc='random seed.'): 42}\n",
            "\n",
            "Decision Tree Metrics:\n",
            "Accuracy: 0.9821428571428571\n",
            "Precision: 0.9835164835164836\n",
            "Recall: 0.9821428571428572\n",
            "F1 Score: 0.9822586872586874\n",
            "\n",
            "Confusion Matrix (Decision Tree):\n",
            " [[25  0  0]\n",
            " [ 0 12  0]\n",
            " [ 0  1 18]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"IrisClassification\").getOrCreate()\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "iris_df['label'] = iris.target\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "df = spark.createDataFrame(iris_df)\n",
        "\n",
        "# Feature Engineering\n",
        "assembler = VectorAssembler(inputCols=iris.feature_names, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Index labels\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
        "df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Define Random Forest classifier\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", seed=42)\n",
        "\n",
        "# Set up pipeline for Random Forest\n",
        "pipeline_rf = Pipeline(stages=[rf])\n",
        "\n",
        "# Create parameter grid for Random Forest\n",
        "paramGrid_rf = (ParamGridBuilder()\n",
        "                .addGrid(rf.numTrees, [10, 15, 20, 25, 30])  # Number of trees in the forest\n",
        "                .addGrid(rf.maxDepth, [5, 7, 10, 12, 15])   # Maximum depth of the tree\n",
        "                .build())\n",
        "\n",
        "# Define cross-validator for Random Forest\n",
        "crossval_rf = CrossValidator(estimator=pipeline_rf,\n",
        "                             estimatorParamMaps=paramGrid_rf,\n",
        "                             evaluator=MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\"),\n",
        "                             numFolds=5, seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Train the Random Forest model using cross-validation\n",
        "cvModel_rf = crossval_rf.fit(train_data)\n",
        "\n",
        "# Make predictions on the test data using Random Forest\n",
        "predictions_rf = cvModel_rf.transform(test_data)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "evaluator_rf = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\")\n",
        "accuracy_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "evaluator_rf.setMetricName(\"weightedPrecision\")\n",
        "precision_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "evaluator_rf.setMetricName(\"weightedRecall\")\n",
        "recall_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "evaluator_rf.setMetricName(\"f1\")\n",
        "f1_score_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "# Print evaluation metrics for Random Forest\n",
        "print(\"Random Forest Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_rf}\")\n",
        "print(f\"Precision: {precision_rf}\")\n",
        "print(f\"Recall: {recall_rf}\")\n",
        "print(f\"F1 Score: {f1_score_rf}\")\n",
        "print()\n",
        "\n",
        "# Compute confusion matrix for Random Forest\n",
        "y_true_rf = predictions_rf.select(\"indexedLabel\").toPandas()\n",
        "y_pred_rf = predictions_rf.select(\"prediction\").toPandas()\n",
        "\n",
        "cm_rf = confusion_matrix(y_true_rf, y_pred_rf)\n",
        "print(\"Confusion Matrix (Random Forest):\\n\", cm_rf)\n",
        "print()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C97o3YumjvtP",
        "outputId": "fd9ede94-e2d1-4c4d-e735-a691bf0e55a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Metrics:\n",
            "Accuracy: 0.9821428571428571\n",
            "Precision: 0.9835164835164836\n",
            "Recall: 0.9821428571428572\n",
            "F1 Score: 0.9822586872586874\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            " [[25  0  0]\n",
            " [ 0 12  0]\n",
            " [ 0  1 18]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"IrisClassification\").getOrCreate()\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "iris_df['label'] = iris.target\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "df = spark.createDataFrame(iris_df)\n",
        "\n",
        "# Feature Engineering\n",
        "assembler = VectorAssembler(inputCols=iris.feature_names, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Index labels\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
        "df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Define Decision Tree classifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", seed=42)\n",
        "\n",
        "# Set up pipeline for Decision Tree\n",
        "pipeline_dt = Pipeline(stages=[dt])\n",
        "\n",
        "# Create parameter grid for Decision Tree\n",
        "paramGrid_dt = (ParamGridBuilder()\n",
        "                .addGrid(dt.maxDepth, [5, 7, 10, 12, 15])  # Maximum depth of the tree\n",
        "                .build())\n",
        "\n",
        "# Define cross-validator for Decision Tree\n",
        "crossval_dt = CrossValidator(estimator=pipeline_dt,\n",
        "                             estimatorParamMaps=paramGrid_dt,\n",
        "                             evaluator=MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\"),\n",
        "                             numFolds=5, seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Train the Decision Tree model using cross-validation\n",
        "cvModel_dt = crossval_dt.fit(train_data)\n",
        "\n",
        "# Make predictions on the test data using Decision Tree\n",
        "predictions_dt = cvModel_dt.transform(test_data)\n",
        "\n",
        "# Evaluate Decision Tree model\n",
        "evaluator_dt = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\")\n",
        "accuracy_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "evaluator_dt.setMetricName(\"weightedPrecision\")\n",
        "precision_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "evaluator_dt.setMetricName(\"weightedRecall\")\n",
        "recall_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "evaluator_dt.setMetricName(\"f1\")\n",
        "f1_score_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "# Print evaluation metrics for Decision Tree\n",
        "print(\"Decision Tree Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_dt}\")\n",
        "print(f\"Precision: {precision_dt}\")\n",
        "print(f\"Recall: {recall_dt}\")\n",
        "print(f\"F1 Score: {f1_score_dt}\")\n",
        "print()\n",
        "\n",
        "# Compute confusion matrix for Decision Tree\n",
        "y_true_dt = predictions_dt.select(\"indexedLabel\").toPandas()\n",
        "y_pred_dt = predictions_dt.select(\"prediction\").toPandas()\n",
        "\n",
        "cm_dt = confusion_matrix(y_true_dt, y_pred_dt)\n",
        "print(\"Confusion Matrix (Decision Tree):\\n\", cm_dt)\n",
        "print()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "vd5HGbtBjvlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **jkdskjdfjkhjdfhjdf**"
      ],
      "metadata": {
        "id": "f3hlT2VFjwdi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59GwdPPxbZaJ",
        "outputId": "d0a7826d-fb73-4774-95e4-be76c6bcb564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest - Best Model Parameters:\n",
            "bootstrap: Whether bootstrap samples are used when building trees. (default: True)\n",
            "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
            "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
            "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\n",
            "featuresCol: features column name. (default: features, current: features)\n",
            "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
            "labelCol: label column name. (default: label, current: indexedLabel)\n",
            "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
            "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
            "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 5)\n",
            "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
            "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
            "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
            "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
            "numTrees: Number of trees to train (>= 1). (default: 20, current: 10)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "seed: random seed. (default: 1393828132015320610, current: 42)\n",
            "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.9821428571428571\n",
            "Precision: 0.9835164835164836\n",
            "Recall: 0.9821428571428572\n",
            "F1 Score: 0.9822586872586874\n",
            "\n",
            "Decision Tree - Best Model Parameters:\n",
            "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
            "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
            "featuresCol: features column name. (default: features, current: features)\n",
            "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
            "labelCol: label column name. (default: label, current: indexedLabel)\n",
            "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
            "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
            "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 5)\n",
            "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
            "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
            "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
            "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "seed: random seed. (default: 8657330483299862127, current: 42)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
            "\n",
            "Decision Tree Metrics:\n",
            "Accuracy: 0.9821428571428571\n",
            "Precision: 0.9835164835164836\n",
            "Recall: 0.9821428571428572\n",
            "F1 Score: 0.9822586872586874\n",
            "\n",
            "Random Forest Predictions:\n",
            "    indexedLabel  prediction\n",
            "0            0.0         0.0\n",
            "1            0.0         0.0\n",
            "2            0.0         0.0\n",
            "3            0.0         0.0\n",
            "4            0.0         0.0\n",
            "5            0.0         0.0\n",
            "6            0.0         0.0\n",
            "7            0.0         0.0\n",
            "8            1.0         1.0\n",
            "9            0.0         0.0\n",
            "10           0.0         0.0\n",
            "11           0.0         0.0\n",
            "12           0.0         0.0\n",
            "13           0.0         0.0\n",
            "14           0.0         0.0\n",
            "15           0.0         0.0\n",
            "16           0.0         0.0\n",
            "17           0.0         0.0\n",
            "18           0.0         0.0\n",
            "19           0.0         0.0\n",
            "20           0.0         0.0\n",
            "21           0.0         0.0\n",
            "22           0.0         0.0\n",
            "23           0.0         0.0\n",
            "24           1.0         1.0\n",
            "25           0.0         0.0\n",
            "26           0.0         0.0\n",
            "27           1.0         1.0\n",
            "28           1.0         1.0\n",
            "29           1.0         1.0\n",
            "30           1.0         1.0\n",
            "31           2.0         1.0\n",
            "32           1.0         1.0\n",
            "33           1.0         1.0\n",
            "34           1.0         1.0\n",
            "35           1.0         1.0\n",
            "36           1.0         1.0\n",
            "37           2.0         2.0\n",
            "38           2.0         2.0\n",
            "39           2.0         2.0\n",
            "40           2.0         2.0\n",
            "41           1.0         1.0\n",
            "42           2.0         2.0\n",
            "43           2.0         2.0\n",
            "44           2.0         2.0\n",
            "45           2.0         2.0\n",
            "46           2.0         2.0\n",
            "47           2.0         2.0\n",
            "48           2.0         2.0\n",
            "49           2.0         2.0\n",
            "50           2.0         2.0\n",
            "51           2.0         2.0\n",
            "52           2.0         2.0\n",
            "53           2.0         2.0\n",
            "54           2.0         2.0\n",
            "55           2.0         2.0\n",
            "\n",
            "Decision Tree Predictions:\n",
            "    indexedLabel  prediction\n",
            "0            0.0         0.0\n",
            "1            0.0         0.0\n",
            "2            0.0         0.0\n",
            "3            0.0         0.0\n",
            "4            0.0         0.0\n",
            "5            0.0         0.0\n",
            "6            0.0         0.0\n",
            "7            0.0         0.0\n",
            "8            1.0         1.0\n",
            "9            0.0         0.0\n",
            "10           0.0         0.0\n",
            "11           0.0         0.0\n",
            "12           0.0         0.0\n",
            "13           0.0         0.0\n",
            "14           0.0         0.0\n",
            "15           0.0         0.0\n",
            "16           0.0         0.0\n",
            "17           0.0         0.0\n",
            "18           0.0         0.0\n",
            "19           0.0         0.0\n",
            "20           0.0         0.0\n",
            "21           0.0         0.0\n",
            "22           0.0         0.0\n",
            "23           0.0         0.0\n",
            "24           1.0         1.0\n",
            "25           0.0         0.0\n",
            "26           0.0         0.0\n",
            "27           1.0         1.0\n",
            "28           1.0         1.0\n",
            "29           1.0         1.0\n",
            "30           1.0         1.0\n",
            "31           2.0         1.0\n",
            "32           1.0         1.0\n",
            "33           1.0         1.0\n",
            "34           1.0         1.0\n",
            "35           1.0         1.0\n",
            "36           1.0         1.0\n",
            "37           2.0         2.0\n",
            "38           2.0         2.0\n",
            "39           2.0         2.0\n",
            "40           2.0         2.0\n",
            "41           1.0         1.0\n",
            "42           2.0         2.0\n",
            "43           2.0         2.0\n",
            "44           2.0         2.0\n",
            "45           2.0         2.0\n",
            "46           2.0         2.0\n",
            "47           2.0         2.0\n",
            "48           2.0         2.0\n",
            "49           2.0         2.0\n",
            "50           2.0         2.0\n",
            "51           2.0         2.0\n",
            "52           2.0         2.0\n",
            "53           2.0         2.0\n",
            "54           2.0         2.0\n",
            "55           2.0         2.0\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"IrisClassification\").getOrCreate()\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "iris_df['label'] = iris.target\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "df = spark.createDataFrame(iris_df)\n",
        "\n",
        "# Feature Engineering\n",
        "assembler = VectorAssembler(inputCols=iris.feature_names, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Index labels\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
        "df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Define Random Forest classifier\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", seed=42)\n",
        "\n",
        "# Set up pipeline for Random Forest\n",
        "pipeline_rf = Pipeline(stages=[rf])\n",
        "\n",
        "# Create parameter grid for Random Forest\n",
        "paramGrid_rf = (ParamGridBuilder()\n",
        "                .addGrid(rf.numTrees, [10, 15, 20, 25, 30])  # Number of trees in the forest\n",
        "                .addGrid(rf.maxDepth, [5, 7, 10, 12, 15])   # Maximum depth of the tree\n",
        "                .build())\n",
        "\n",
        "# Define cross-validator for Random Forest\n",
        "crossval_rf = CrossValidator(estimator=pipeline_rf,\n",
        "                             estimatorParamMaps=paramGrid_rf,\n",
        "                             evaluator=MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\"),\n",
        "                             numFolds=5, seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Train the Random Forest model using cross-validation\n",
        "cvModel_rf = crossval_rf.fit(train_data)\n",
        "\n",
        "# Print best model parameters for Random Forest\n",
        "# print(\"Random Forest - Best Model Parameters:\")\n",
        "# best_rf_model = cvModel_rf.bestModel.stages[-1]\n",
        "# print(best_rf_model.explainParams())\n",
        "# print()\n",
        "\n",
        "# Make predictions on the test data using Random Forest\n",
        "predictions_rf = cvModel_rf.transform(test_data)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "evaluator_rf = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\")\n",
        "accuracy_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "evaluator_rf.setMetricName(\"weightedPrecision\")\n",
        "precision_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "evaluator_rf.setMetricName(\"weightedRecall\")\n",
        "recall_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "evaluator_rf.setMetricName(\"f1\")\n",
        "f1_score_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "# Print evaluation metrics for Random Forest\n",
        "print(\"Random Forest Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_rf}\")\n",
        "print(f\"Precision: {precision_rf}\")\n",
        "print(f\"Recall: {recall_rf}\")\n",
        "print(f\"F1 Score: {f1_score_rf}\")\n",
        "print()\n",
        "\n",
        "# Define Decision Tree classifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", seed=42)\n",
        "\n",
        "# Set up pipeline for Decision Tree\n",
        "pipeline_dt = Pipeline(stages=[dt])\n",
        "\n",
        "# Create parameter grid for Decision Tree\n",
        "paramGrid_dt = (ParamGridBuilder()\n",
        "                .addGrid(dt.maxDepth, [5, 7, 10, 12, 15])  # Maximum depth of the tree\n",
        "                .build())\n",
        "\n",
        "# Define cross-validator for Decision Tree\n",
        "crossval_dt = CrossValidator(estimator=pipeline_dt,\n",
        "                             estimatorParamMaps=paramGrid_dt,\n",
        "                             evaluator=MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\"),\n",
        "                             numFolds=5, seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Train the Decision Tree model using cross-validation\n",
        "cvModel_dt = crossval_dt.fit(train_data)\n",
        "\n",
        "# Print best model parameters for Decision Tree\n",
        "# print(\"Decision Tree - Best Model Parameters:\")\n",
        "# best_dt_model = cvModel_dt.bestModel.stages[-1]\n",
        "# print(best_dt_model.explainParams())\n",
        "# print()\n",
        "\n",
        "# Make predictions on the test data using Decision Tree\n",
        "predictions_dt = cvModel_dt.transform(test_data)\n",
        "\n",
        "# Evaluate Decision Tree model\n",
        "evaluator_dt = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\")\n",
        "accuracy_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "evaluator_dt.setMetricName(\"weightedPrecision\")\n",
        "precision_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "evaluator_dt.setMetricName(\"weightedRecall\")\n",
        "recall_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "evaluator_dt.setMetricName(\"f1\")\n",
        "f1_score_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "# Print evaluation metrics for Decision Tree\n",
        "print(\"Decision Tree Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_dt}\")\n",
        "print(f\"Precision: {precision_dt}\")\n",
        "print(f\"Recall: {recall_dt}\")\n",
        "print(f\"F1 Score: {f1_score_dt}\")\n",
        "\n",
        "# Convert predictions to Pandas DataFrame for better readability\n",
        "predictions_rf_df = predictions_rf.select(\"indexedLabel\", \"prediction\").toPandas()\n",
        "predictions_dt_df = predictions_dt.select(\"indexedLabel\", \"prediction\").toPandas()\n",
        "\n",
        "# Print predicted and actual labels for Random Forest\n",
        "print(\"\\nRandom Forest Predictions:\")\n",
        "print(predictions_rf_df)\n",
        "\n",
        "# Print predicted and actual labels for Decision Tree\n",
        "print(\"\\nDecision Tree Predictions:\")\n",
        "print(predictions_dt_df)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vBiZCeCc99g",
        "outputId": "d32275dc-6dd3-46ff-81df-2166ea254521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest - Best Model Parameters:\n",
            "bootstrap: Whether bootstrap samples are used when building trees. (default: True)\n",
            "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
            "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
            "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\n",
            "featuresCol: features column name. (default: features, current: features)\n",
            "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
            "labelCol: label column name. (default: label, current: indexedLabel)\n",
            "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
            "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
            "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 5)\n",
            "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
            "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
            "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
            "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
            "numTrees: Number of trees to train (>= 1). (default: 20, current: 10)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "seed: random seed. (default: 1393828132015320610, current: 42)\n",
            "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.9821428571428571\n",
            "Precision: 0.9835164835164836\n",
            "Recall: 0.9821428571428572\n",
            "F1 Score: 0.9822586872586874\n",
            "\n",
            "\n",
            "Random Forest Predictions:\n",
            "    indexedLabel  prediction\n",
            "0            0.0         0.0\n",
            "1            0.0         0.0\n",
            "2            0.0         0.0\n",
            "3            0.0         0.0\n",
            "4            0.0         0.0\n",
            "5            0.0         0.0\n",
            "6            0.0         0.0\n",
            "7            0.0         0.0\n",
            "8            1.0         1.0\n",
            "9            0.0         0.0\n",
            "10           0.0         0.0\n",
            "11           0.0         0.0\n",
            "12           0.0         0.0\n",
            "13           0.0         0.0\n",
            "14           0.0         0.0\n",
            "15           0.0         0.0\n",
            "16           0.0         0.0\n",
            "17           0.0         0.0\n",
            "18           0.0         0.0\n",
            "19           0.0         0.0\n",
            "20           0.0         0.0\n",
            "21           0.0         0.0\n",
            "22           0.0         0.0\n",
            "23           0.0         0.0\n",
            "24           1.0         1.0\n",
            "25           0.0         0.0\n",
            "26           0.0         0.0\n",
            "27           1.0         1.0\n",
            "28           1.0         1.0\n",
            "29           1.0         1.0\n",
            "30           1.0         1.0\n",
            "31           2.0         1.0\n",
            "32           1.0         1.0\n",
            "33           1.0         1.0\n",
            "34           1.0         1.0\n",
            "35           1.0         1.0\n",
            "36           1.0         1.0\n",
            "37           2.0         2.0\n",
            "38           2.0         2.0\n",
            "39           2.0         2.0\n",
            "40           2.0         2.0\n",
            "41           1.0         1.0\n",
            "42           2.0         2.0\n",
            "43           2.0         2.0\n",
            "44           2.0         2.0\n",
            "45           2.0         2.0\n",
            "46           2.0         2.0\n",
            "47           2.0         2.0\n",
            "48           2.0         2.0\n",
            "49           2.0         2.0\n",
            "50           2.0         2.0\n",
            "51           2.0         2.0\n",
            "52           2.0         2.0\n",
            "53           2.0         2.0\n",
            "54           2.0         2.0\n",
            "55           2.0         2.0\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"RandomForestIrisClassification\").getOrCreate()\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "iris_df['label'] = iris.target\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "df = spark.createDataFrame(iris_df)\n",
        "\n",
        "# Feature Engineering\n",
        "assembler = VectorAssembler(inputCols=iris.feature_names, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Index labels\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
        "df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Define Random Forest classifier\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", seed=42)\n",
        "\n",
        "# Set up pipeline for Random Forest\n",
        "pipeline_rf = Pipeline(stages=[rf])\n",
        "\n",
        "# Create parameter grid for Random Forest\n",
        "paramGrid_rf = (ParamGridBuilder()\n",
        "                .addGrid(rf.numTrees, [10, 15, 20, 25, 30])  # Number of trees in the forest\n",
        "                .addGrid(rf.maxDepth, [5, 7, 10, 12, 15])   # Maximum depth of the tree\n",
        "                .build())\n",
        "\n",
        "# Define cross-validator for Random Forest\n",
        "crossval_rf = CrossValidator(estimator=pipeline_rf,\n",
        "                             estimatorParamMaps=paramGrid_rf,\n",
        "                             evaluator=MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\"),\n",
        "                             numFolds=5, seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Train the Random Forest model using cross-validation\n",
        "cvModel_rf = crossval_rf.fit(train_data)\n",
        "\n",
        "# Print best model parameters for Random Forest\n",
        "print(\"Random Forest - Best Model Parameters:\")\n",
        "best_rf_model = cvModel_rf.bestModel.stages[-1]\n",
        "print(best_rf_model.explainParams())\n",
        "print()\n",
        "\n",
        "# Make predictions on the test data using Random Forest\n",
        "predictions_rf = cvModel_rf.transform(test_data)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "evaluator_rf = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\")\n",
        "accuracy_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "evaluator_rf.setMetricName(\"weightedPrecision\")\n",
        "precision_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "evaluator_rf.setMetricName(\"weightedRecall\")\n",
        "recall_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "evaluator_rf.setMetricName(\"f1\")\n",
        "f1_score_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "\n",
        "# Print evaluation metrics for Random Forest\n",
        "print(\"Random Forest Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_rf}\")\n",
        "print(f\"Precision: {precision_rf}\")\n",
        "print(f\"Recall: {recall_rf}\")\n",
        "print(f\"F1 Score: {f1_score_rf}\")\n",
        "print()\n",
        "\n",
        "# Convert predictions to Pandas DataFrame for better readability\n",
        "predictions_rf_df = predictions_rf.select(\"indexedLabel\", \"prediction\").toPandas()\n",
        "\n",
        "# Print predicted and actual labels for Random Forest\n",
        "print(\"\\nRandom Forest Predictions:\")\n",
        "print(predictions_rf_df)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIF_Sexrc90p",
        "outputId": "253cc80a-97d1-42cc-b8b6-c2f30e1ce441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree - Best Model Parameters:\n",
            "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
            "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
            "featuresCol: features column name. (default: features, current: features)\n",
            "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
            "labelCol: label column name. (default: label, current: indexedLabel)\n",
            "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
            "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
            "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 5)\n",
            "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
            "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
            "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
            "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "seed: random seed. (default: 8657330483299862127, current: 42)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
            "\n",
            "Decision Tree Metrics:\n",
            "Accuracy: 0.9821428571428571\n",
            "Precision: 0.9835164835164836\n",
            "Recall: 0.9821428571428572\n",
            "F1 Score: 0.9822586872586874\n",
            "\n",
            "\n",
            "Decision Tree Predictions:\n",
            "    indexedLabel  prediction\n",
            "0            0.0         0.0\n",
            "1            0.0         0.0\n",
            "2            0.0         0.0\n",
            "3            0.0         0.0\n",
            "4            0.0         0.0\n",
            "5            0.0         0.0\n",
            "6            0.0         0.0\n",
            "7            0.0         0.0\n",
            "8            1.0         1.0\n",
            "9            0.0         0.0\n",
            "10           0.0         0.0\n",
            "11           0.0         0.0\n",
            "12           0.0         0.0\n",
            "13           0.0         0.0\n",
            "14           0.0         0.0\n",
            "15           0.0         0.0\n",
            "16           0.0         0.0\n",
            "17           0.0         0.0\n",
            "18           0.0         0.0\n",
            "19           0.0         0.0\n",
            "20           0.0         0.0\n",
            "21           0.0         0.0\n",
            "22           0.0         0.0\n",
            "23           0.0         0.0\n",
            "24           1.0         1.0\n",
            "25           0.0         0.0\n",
            "26           0.0         0.0\n",
            "27           1.0         1.0\n",
            "28           1.0         1.0\n",
            "29           1.0         1.0\n",
            "30           1.0         1.0\n",
            "31           2.0         1.0\n",
            "32           1.0         1.0\n",
            "33           1.0         1.0\n",
            "34           1.0         1.0\n",
            "35           1.0         1.0\n",
            "36           1.0         1.0\n",
            "37           2.0         2.0\n",
            "38           2.0         2.0\n",
            "39           2.0         2.0\n",
            "40           2.0         2.0\n",
            "41           1.0         1.0\n",
            "42           2.0         2.0\n",
            "43           2.0         2.0\n",
            "44           2.0         2.0\n",
            "45           2.0         2.0\n",
            "46           2.0         2.0\n",
            "47           2.0         2.0\n",
            "48           2.0         2.0\n",
            "49           2.0         2.0\n",
            "50           2.0         2.0\n",
            "51           2.0         2.0\n",
            "52           2.0         2.0\n",
            "53           2.0         2.0\n",
            "54           2.0         2.0\n",
            "55           2.0         2.0\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"DecisionTreeIrisClassification\").getOrCreate()\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "iris_df['label'] = iris.target\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "df = spark.createDataFrame(iris_df)\n",
        "\n",
        "# Feature Engineering\n",
        "assembler = VectorAssembler(inputCols=iris.feature_names, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Index labels\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
        "df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Define Decision Tree classifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", seed=42)\n",
        "\n",
        "# Set up pipeline for Decision Tree\n",
        "pipeline_dt = Pipeline(stages=[dt])\n",
        "\n",
        "# Create parameter grid for Decision Tree\n",
        "paramGrid_dt = (ParamGridBuilder()\n",
        "                .addGrid(dt.maxDepth, [5, 7, 10, 12, 15])  # Maximum depth of the tree\n",
        "                .build())\n",
        "\n",
        "# Define cross-validator for Decision Tree\n",
        "crossval_dt = CrossValidator(estimator=pipeline_dt,\n",
        "                             estimatorParamMaps=paramGrid_dt,\n",
        "                             evaluator=MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\"),\n",
        "                             numFolds=5, seed=42)  # Set seed for reproducibility\n",
        "\n",
        "# Train the Decision Tree model using cross-validation\n",
        "cvModel_dt = crossval_dt.fit(train_data)\n",
        "\n",
        "# Print best model parameters for Decision Tree\n",
        "print(\"Decision Tree - Best Model Parameters:\")\n",
        "best_dt_model = cvModel_dt.bestModel.stages[-1]\n",
        "print(best_dt_model.explainParams())\n",
        "print()\n",
        "\n",
        "# Make predictions on the test data using Decision Tree\n",
        "predictions_dt = cvModel_dt.transform(test_data)\n",
        "\n",
        "# Evaluate Decision Tree model\n",
        "evaluator_dt = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", metricName=\"accuracy\")\n",
        "accuracy_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "evaluator_dt.setMetricName(\"weightedPrecision\")\n",
        "precision_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "evaluator_dt.setMetricName(\"weightedRecall\")\n",
        "recall_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "evaluator_dt.setMetricName(\"f1\")\n",
        "f1_score_dt = evaluator_dt.evaluate(predictions_dt)\n",
        "\n",
        "# Print evaluation metrics for Decision Tree\n",
        "print(\"Decision Tree Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_dt}\")\n",
        "print(f\"Precision: {precision_dt}\")\n",
        "print(f\"Recall: {recall_dt}\")\n",
        "print(f\"F1 Score: {f1_score_dt}\")\n",
        "print()\n",
        "\n",
        "# Convert predictions to Pandas DataFrame for better readability\n",
        "predictions_dt_df = predictions_dt.select(\"indexedLabel\", \"prediction\").toPandas()\n",
        "\n",
        "# Print predicted and actual labels for Decision Tree\n",
        "print(\"\\nDecision Tree Predictions:\")\n",
        "print(predictions_dt_df)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}